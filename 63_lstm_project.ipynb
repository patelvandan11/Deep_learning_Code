{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IrmA3eaj3kF0"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"\n",
        " Multi-headed attention\n",
        " In our simple example, we only used the embeddings “as is” to compute the attention\n",
        " scores and weights, but that’s far from the whole story. In practice, the self-attention\n",
        " layer applies three independent linear transformations to each embedding to generate\n",
        " the query, key, and value vectors. These transformations project the embeddings and\n",
        " each projection carries its own set of learnable parameters, which allows the self\n",
        "attention layer to focus on different semantic aspects of the sequence.\n",
        " It also turns out to be beneficial to have multiple sets of linear projections, each one\n",
        " representing a so-called attention head. The resulting multi-head attention layer is\n",
        " illustrated in Figure 3-5. But why do we need more than one attention head? The rea\n",
        "son is that the softmax of one head tends to focus on mostly one aspect of similarity.\n",
        " Having several heads allows the model to focus on several aspects at once. For\n",
        " instance, one head can focus on subject-verb interaction, whereas another finds\n",
        " nearby adjectives. Obviously we don’t handcraft these relations into the model, and\n",
        " they are fully learned from the data. If you are familiar with computer vision models\n",
        " you might see the resemblance to filters in convolutional neural networks, where one\n",
        " filter can be responsible for detecting faces and another one finds wheels of cars in\n",
        " images.\n",
        " The Encoder \n",
        "| \n",
        "67\n",
        "Figure 3-5. Multi-head attention\n",
        " Let’s implement this layer by first coding up a single attention head:\n",
        " class AttentionHead(nn.Module):\n",
        " def __init__(self, embed_dim, head_dim):\n",
        " super().__init__()\n",
        " self.q = nn.Linear(embed_dim, head_dim)\n",
        " self.k = nn.Linear(embed_dim, head_dim)\n",
        " self.v = nn.Linear(embed_dim, head_dim)\n",
        " def forward(self, hidden_state):\n",
        " attn_outputs = scaled_dot_product_attention(\n",
        " self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        " return attn_outputs\n",
        " Here we’ve initialized three independent linear layers that apply matrix multiplication\n",
        " to the embedding vectors to produce tensors of shape [batch_size, seq_len,\n",
        " head_dim], where head_dim is the number of dimensions we are projecting into.\n",
        " Although head_dim does not have to be smaller than the number of embedding\n",
        " dimensions of the tokens (embed_dim), in practice it is chosen to be a multiple of\n",
        " embed_dim so that the computation across each head is constant. For example, BERT\n",
        " has 12 attention heads, so the dimension of each head is 768/12 = 64.\n",
        " Now that we have a single attention head, we can concatenate the outputs of each one\n",
        " to implement the full multi-head attention layer:\n",
        " class MultiHeadAttention(nn.Module):\n",
        " def __init__(self, config):\n",
        " super().__init__()\n",
        " embed_dim = config.hidden_size\n",
        " num_heads = config.num_attention_heads\n",
        " head_dim = embed_dim // num_heads\n",
        " self.heads = nn.ModuleList(\n",
        " [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        " )\n",
        " self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        " 68 \n",
        "| \n",
        "Chapter 3: Transformer Anatomy\n",
        "def forward(self, hidden_state):\n",
        " x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        " x = self.output_linear(x)\n",
        " return x\n",
        " Notice that the concatenated output from the attention heads is also fed through a\n",
        " final linear layer to produce an output tensor of shape [batch_size, seq_len,\n",
        " hidden_dim] that is suitable for the feed-forward network downstream. To confirm,\n",
        " let’s see if the multi-head attention layer produces the expected shape of our inputs.\n",
        " We pass the configuration we loaded earlier from the pretrained BERT model when\n",
        " initializing the MultiHeadAttention module. This ensures that we use the same set\n",
        "tings as BERT:\n",
        " multihead_attn = MultiHeadAttention(config)\n",
        " attn_output = multihead_attn(inputs_embeds)\n",
        " attn_output.size()\n",
        " torch.Size([1, 5, 768])\n",
        " It works! To wrap up this section on attention, let’s use BertViz again to visualize the\n",
        " attention for two different uses of the word “flies”. Here we can use the head_view()\n",
        " function from BertViz by computing the attentions of a pretrained checkpoint and\n",
        " indicating where the sentence boundary lies:\n",
        "\n",
        " from bertviz import head_view\n",
        " from transformers import AutoModel\n",
        " model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n",
        " sentence_a = \"time flies like an arrow\"\n",
        " sentence_b = \"fruit flies like a banana\"\n",
        " viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
        " attention = model(**viz_inputs).attentions\n",
        " sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n",
        " tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n",
        " head_view(attention, tokens, sentence_b_start, heads=[8])\n",
        " \n",
        " The Encoder \n",
        "| \n",
        "69\n",
        "This visualization shows the attention weights as lines connecting the token whose\n",
        " embedding is getting updated (left) with every word that is being attended to (right).\n",
        " The intensity of the lines indicates the strength of the attention weights, with dark\n",
        " lines representing values close to 1, and faint lines representing values close to 0.\n",
        " In this example, the input consists of two sentences and the [CLS] and [SEP] tokens\n",
        " are the special tokens in BERT’s tokenizer that we encountered in Chapter 2. One\n",
        " thing we can see from the visualization is that the attention weights are strongest\n",
        " between words that belong to the same sentence, which suggests BERT can tell that it\n",
        " should attend to words in the same sentence. However, for the word “flies” we can see\n",
        " that BERT has identified “arrow” as important in the first sentence and “fruit” and\n",
        " “banana” in the second. These attention weights allow the model to distinguish the\n",
        " use of “flies” as a verb or noun, depending on the context in which it occurs!\n",
        " Now that we’ve covered attention, let’s take a look at implementing the missing piece\n",
        " of the encoder layer: position-wise feed-forward networks.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J1D42emD32Ro"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KhtDxwL_AXFj"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K8MRFre9AaG9"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts([faqs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrpAl3EDAgvh",
        "outputId": "cabe3c83-7274-4512-94d5-b5def2b72453"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "354"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "44VahqKdAjr9"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for sentence in faqs.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyqwPDzNA5mR",
        "outputId": "98e99353-46c5-4252-c280-85f8eeef1dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[33, 150],\n",
              " [33, 150, 2],\n",
              " [8, 83],\n",
              " [8, 83, 151],\n",
              " [8, 83, 151, 57],\n",
              " [8, 83, 151, 57, 10],\n",
              " [8, 83, 151, 57, 10, 152],\n",
              " [8, 83, 151, 57, 10, 152, 153],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84, 154],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84, 154, 155],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84, 154, 155, 3],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84, 154, 155, 3, 156],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84, 154, 155, 3, 156, 1],\n",
              " [8, 83, 151, 57, 10, 152, 153, 1, 84, 154, 155, 3, 156, 1, 2],\n",
              " [157, 11],\n",
              " [157, 11, 34],\n",
              " [157, 11, 34, 85],\n",
              " [157, 11, 34, 85, 158],\n",
              " [157, 11, 34, 85, 158, 159],\n",
              " [157, 11, 34, 85, 158, 159, 18],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160, 161],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160, 161, 8],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160, 161, 8, 86],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160, 161, 8, 86, 1],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160, 161, 8, 86, 1, 7],\n",
              " [157, 11, 34, 85, 158, 159, 18, 1, 160, 161, 8, 86, 1, 7, 2],\n",
              " [20, 162],\n",
              " [20, 162, 87],\n",
              " [20, 162, 87, 88],\n",
              " [20, 162, 87, 88, 15],\n",
              " [20, 162, 87, 88, 15, 89],\n",
              " [20, 162, 87, 88, 15, 89, 3],\n",
              " [20, 162, 87, 88, 15, 89, 3, 27],\n",
              " [20, 162, 87, 88, 15, 89, 3, 27, 40],\n",
              " [20, 162, 87, 88, 15, 89, 3, 27, 40, 3],\n",
              " [20, 162, 87, 88, 15, 89, 3, 27, 40, 3, 163],\n",
              " [1, 164],\n",
              " [1, 164, 165],\n",
              " [1, 164, 165, 11],\n",
              " [1, 164, 165, 11, 166],\n",
              " [1, 164, 165, 11, 166, 90],\n",
              " [1, 164, 165, 11, 166, 90, 58],\n",
              " [1, 164, 165, 11, 166, 90, 58, 89],\n",
              " [1, 164, 165, 11, 166, 90, 58, 89, 167],\n",
              " [1, 164, 165, 11, 166, 90, 58, 89, 167, 1],\n",
              " [1, 164, 165, 11, 166, 90, 58, 89, 167, 1, 84],\n",
              " [1, 164, 165, 11, 166, 90, 58, 89, 167, 1, 84, 11],\n",
              " [27, 168],\n",
              " [27, 168, 169],\n",
              " [27, 168, 169, 170],\n",
              " [27, 168, 169, 170, 171],\n",
              " [27, 168, 169, 170, 171, 91],\n",
              " [27, 168, 169, 170, 171, 91, 6],\n",
              " [27, 168, 169, 170, 171, 91, 6, 172],\n",
              " [27, 168, 169, 170, 171, 91, 6, 172, 173],\n",
              " [27, 168, 169, 170, 171, 91, 6, 172, 173, 59],\n",
              " [27, 168, 169, 170, 171, 91, 6, 172, 173, 59, 92],\n",
              " [27, 168, 169, 170, 171, 91, 6, 172, 173, 59, 92, 1],\n",
              " [27, 168, 169, 170, 171, 91, 6, 172, 173, 59, 92, 1, 7],\n",
              " [2, 20],\n",
              " [2, 20, 3],\n",
              " [2, 20, 3, 41],\n",
              " [2, 20, 3, 41, 28],\n",
              " [2, 20, 3, 41, 28, 93],\n",
              " [2, 20, 3, 41, 28, 93, 174],\n",
              " [2, 20, 3, 41, 28, 93, 174, 94],\n",
              " [2, 20, 3, 41, 28, 93, 174, 94, 6],\n",
              " [2, 20, 3, 41, 28, 93, 174, 94, 6, 1],\n",
              " [2, 20, 3, 41, 28, 93, 174, 94, 6, 1, 175],\n",
              " [35, 95],\n",
              " [35, 95, 176],\n",
              " [35, 95, 176, 177],\n",
              " [35, 95, 176, 177, 3],\n",
              " [35, 95, 176, 177, 3, 42],\n",
              " [35, 95, 176, 177, 3, 42, 178],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96, 179],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96, 179, 6],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96, 179, 6, 15],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96, 179, 6, 15, 180],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96, 179, 6, 15, 180, 27],\n",
              " [35, 95, 176, 177, 3, 42, 178, 3, 60, 96, 179, 6, 15, 180, 27, 19],\n",
              " [61, 12],\n",
              " [61, 12, 62],\n",
              " [61, 12, 62, 181],\n",
              " [61, 12, 62, 181, 2],\n",
              " [61, 12, 62, 181, 2, 4],\n",
              " [61, 12, 62, 181, 2, 4, 1],\n",
              " [61, 12, 62, 181, 2, 4, 1, 182],\n",
              " [61, 12, 62, 181, 2, 4, 1, 182, 33],\n",
              " [61, 12, 62, 181, 2, 4, 1, 182, 33, 4],\n",
              " [61, 12, 62, 181, 2, 4, 1, 182, 33, 4, 2],\n",
              " [61, 12, 62, 181, 2, 4, 1, 182, 33, 4, 2, 20],\n",
              " [61, 12, 62, 181, 2, 4, 1, 182, 33, 4, 2, 20, 13],\n",
              " [183, 8],\n",
              " [183, 8, 97],\n",
              " [183, 8, 97, 63],\n",
              " [183, 8, 97, 63, 64],\n",
              " [183, 8, 97, 63, 64, 85],\n",
              " [183, 8, 97, 63, 64, 85, 184],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187, 98],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187, 98, 19],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187, 98, 19, 2],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187, 98, 19, 2, 4],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187, 98, 19, 2, 4, 1],\n",
              " [183, 8, 97, 63, 64, 85, 184, 185, 10, 186, 187, 98, 19, 2, 4, 1, 188],\n",
              " [189, 13],\n",
              " [189, 13, 9],\n",
              " [189, 13, 9, 1],\n",
              " [189, 13, 9, 1, 190],\n",
              " [189, 13, 9, 1, 190, 6],\n",
              " [189, 13, 9, 1, 190, 6, 19],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41, 28],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41, 28, 192],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41, 28, 192, 19],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41, 28, 192, 19, 193],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41, 28, 192, 19, 193, 6],\n",
              " [189, 13, 9, 1, 190, 6, 19, 4, 191, 3, 41, 28, 192, 19, 193, 6, 194],\n",
              " [195, 99],\n",
              " [195, 99, 16],\n",
              " [195, 99, 16, 92],\n",
              " [195, 99, 16, 92, 1],\n",
              " [195, 99, 16, 92, 1, 23],\n",
              " [195, 99, 16, 92, 1, 23, 3],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41, 28],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41, 28, 99],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41, 28, 99, 94],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41, 28, 99, 94, 100],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41, 28, 99, 94, 100, 196],\n",
              " [195, 99, 16, 92, 1, 23, 3, 41, 28, 99, 94, 100, 196, 21],\n",
              " [197, 19],\n",
              " [197, 19, 4],\n",
              " [197, 19, 4, 24],\n",
              " [197, 19, 4, 24, 41],\n",
              " [197, 19, 4, 24, 41, 28],\n",
              " [197, 19, 4, 24, 41, 28, 198],\n",
              " [197, 19, 4, 24, 41, 28, 198, 101],\n",
              " [197, 19, 4, 24, 41, 28, 198, 101, 199],\n",
              " [197, 19, 4, 24, 41, 28, 198, 101, 199, 200],\n",
              " [197, 19, 4, 24, 41, 28, 198, 101, 199, 200, 102],\n",
              " [197, 19, 4, 24, 41, 28, 198, 101, 199, 200, 102, 103],\n",
              " [201, 202],\n",
              " [201, 202, 203],\n",
              " [201, 202, 203, 10],\n",
              " [201, 202, 203, 10, 204],\n",
              " [201, 202, 203, 10, 204, 205],\n",
              " [201, 202, 203, 10, 204, 205, 58],\n",
              " [201, 202, 203, 10, 204, 205, 58, 206],\n",
              " [201, 202, 203, 10, 204, 205, 58, 206, 104],\n",
              " [201, 202, 203, 10, 204, 205, 58, 206, 104, 1],\n",
              " [201, 202, 203, 10, 204, 205, 58, 206, 104, 1, 23],\n",
              " [201, 202, 203, 10, 204, 205, 58, 206, 104, 1, 23, 11],\n",
              " [207, 36],\n",
              " [207, 36, 208],\n",
              " [207, 36, 208, 209],\n",
              " [207, 36, 208, 209, 18],\n",
              " [207, 36, 208, 209, 18, 1],\n",
              " [207, 36, 208, 209, 18, 1, 210],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106, 36],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106, 36, 211],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106, 36, 211, 65],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106, 36, 211, 65, 212],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106, 36, 211, 65, 212, 213],\n",
              " [207, 36, 208, 209, 18, 1, 210, 105, 106, 36, 211, 65, 212, 213, 214],\n",
              " [106, 215],\n",
              " [106, 215, 43],\n",
              " [106, 215, 43, 1],\n",
              " [106, 215, 43, 1, 216],\n",
              " [106, 215, 43, 1, 216, 3],\n",
              " [106, 215, 43, 1, 216, 3, 217],\n",
              " [106, 215, 43, 1, 216, 3, 217, 8],\n",
              " [106, 215, 43, 1, 216, 3, 217, 8, 218],\n",
              " [106, 215, 43, 1, 216, 3, 217, 8, 218, 219],\n",
              " [106, 215, 43, 1, 216, 3, 217, 8, 218, 219, 107],\n",
              " [106, 215, 43, 1, 216, 3, 217, 8, 218, 219, 107, 66],\n",
              " [106, 215, 43, 1, 216, 3, 217, 8, 218, 219, 107, 66, 19],\n",
              " [220, 24],\n",
              " [220, 24, 42],\n",
              " [220, 24, 42, 221],\n",
              " [220, 24, 42, 221, 21],\n",
              " [220, 24, 42, 221, 21, 222],\n",
              " [220, 24, 42, 221, 21, 222, 223],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102, 19],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102, 19, 103],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102, 19, 103, 224],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102, 19, 103, 224, 6],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102, 19, 103, 224, 6, 225],\n",
              " [220, 24, 42, 221, 21, 222, 223, 11, 102, 19, 103, 224, 6, 225, 8],\n",
              " [1, 67],\n",
              " [97, 63],\n",
              " [97, 63, 64],\n",
              " [97, 63, 64, 33],\n",
              " [97, 63, 64, 33, 4],\n",
              " [97, 63, 64, 33, 4, 2],\n",
              " [44, 108],\n",
              " [44, 108, 37],\n",
              " [44, 108, 37, 20],\n",
              " [44, 108, 37, 20, 109],\n",
              " [44, 108, 37, 20, 109, 110],\n",
              " [44, 108, 37, 20, 109, 110, 228],\n",
              " [44, 108, 37, 20, 109, 110, 228, 111],\n",
              " [44, 108, 37, 20, 109, 110, 228, 111, 12],\n",
              " [44, 108, 37, 20, 109, 110, 228, 111, 12, 112],\n",
              " [44, 108, 37, 20, 109, 110, 228, 111, 12, 112, 2],\n",
              " [44, 108, 37, 20, 109, 110, 228, 111, 12, 112, 2, 4],\n",
              " [113, 114],\n",
              " [113, 114, 25],\n",
              " [113, 114, 25, 68],\n",
              " [45, 46],\n",
              " [45, 46, 7],\n",
              " [45, 46, 7, 14],\n",
              " [45, 46, 7, 14, 5],\n",
              " [45, 46, 7, 14, 5, 4],\n",
              " [45, 46, 7, 14, 5, 4, 5],\n",
              " [115, 46],\n",
              " [7, 116],\n",
              " [7, 116, 25],\n",
              " [7, 116, 25, 15],\n",
              " [7, 116, 25, 15, 14],\n",
              " [7, 116, 25, 15, 14, 5],\n",
              " [7, 116, 25, 15, 14, 5, 4],\n",
              " [7, 116, 25, 15, 14, 5, 4, 5],\n",
              " [7, 117],\n",
              " [7, 117, 25],\n",
              " [7, 117, 25, 15],\n",
              " [7, 117, 25, 15, 14],\n",
              " [7, 117, 25, 15, 14, 5],\n",
              " [7, 117, 25, 15, 14, 5, 4],\n",
              " [7, 117, 25, 15, 14, 5, 4, 5],\n",
              " [7, 118],\n",
              " [7, 118, 25],\n",
              " [7, 118, 25, 15],\n",
              " [7, 118, 25, 15, 14],\n",
              " [7, 118, 25, 15, 14, 5],\n",
              " [7, 118, 25, 15, 14, 5, 4],\n",
              " [7, 118, 25, 15, 14, 5, 4, 5],\n",
              " [45, 47],\n",
              " [45, 47, 7],\n",
              " [45, 47, 7, 22],\n",
              " [45, 47, 7, 22, 29],\n",
              " [30, 69],\n",
              " [30, 69, 229],\n",
              " [30, 69, 229, 230],\n",
              " [30, 69, 229, 230, 231],\n",
              " [30, 69, 229, 230, 231, 2],\n",
              " [7, 116],\n",
              " [7, 116, 22],\n",
              " [7, 116, 22, 29],\n",
              " [7, 116, 22, 29, 7],\n",
              " [7, 116, 22, 29, 7, 117],\n",
              " [7, 116, 22, 29, 7, 117, 22],\n",
              " [7, 116, 22, 29, 7, 117, 22, 29],\n",
              " [7, 116, 22, 29, 7, 117, 22, 29, 7],\n",
              " [7, 116, 22, 29, 7, 117, 22, 29, 7, 118],\n",
              " [7, 116, 22, 29, 7, 117, 22, 29, 7, 118, 22],\n",
              " [7, 116, 22, 29, 7, 117, 22, 29, 7, 118, 22, 29],\n",
              " [70, 30],\n",
              " [70, 30, 69],\n",
              " [119, 120],\n",
              " [119, 120, 232],\n",
              " [119, 120, 232, 87],\n",
              " [119, 120, 232, 87, 88],\n",
              " [119, 120, 232, 87, 88, 15],\n",
              " [119, 120, 232, 87, 88, 15, 233],\n",
              " [119, 120, 232, 87, 88, 15, 233, 9],\n",
              " [119, 120, 232, 87, 88, 15, 233, 9, 234],\n",
              " [119, 120, 232, 87, 88, 15, 233, 9, 234, 235],\n",
              " [119, 120, 232, 87, 88, 15, 233, 9, 234, 235, 236],\n",
              " [3, 1],\n",
              " [3, 1, 40],\n",
              " [3, 1, 40, 90],\n",
              " [3, 1, 40, 90, 3],\n",
              " [3, 1, 40, 90, 3, 121],\n",
              " [3, 1, 40, 90, 3, 121, 122],\n",
              " [3, 1, 40, 90, 3, 121, 122, 6],\n",
              " [3, 1, 40, 90, 3, 121, 122, 6, 71],\n",
              " [3, 1, 40, 90, 3, 121, 122, 6, 71, 123],\n",
              " [3, 1, 40, 90, 3, 121, 122, 6, 71, 123, 38],\n",
              " [3, 1, 40, 90, 3, 121, 122, 6, 71, 123, 38, 124],\n",
              " [3, 1, 40, 90, 3, 121, 122, 6, 71, 123, 38, 124, 125],\n",
              " [4, 5],\n",
              " [4, 5, 66],\n",
              " [4, 5, 66, 4],\n",
              " [4, 5, 66, 4, 5],\n",
              " [4, 5, 66, 4, 5, 13],\n",
              " [4, 5, 66, 4, 5, 13, 1],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126, 6],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126, 6, 127],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126, 6, 127, 10],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126, 6, 127, 10, 36],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126, 6, 127, 10, 36, 237],\n",
              " [4, 5, 66, 4, 5, 13, 1, 126, 6, 127, 10, 36, 237, 104],\n",
              " [238, 4],\n",
              " [238, 4, 5],\n",
              " [238, 4, 5, 239],\n",
              " [238, 4, 5, 239, 240],\n",
              " [238, 4, 5, 239, 240, 60],\n",
              " [238, 4, 5, 239, 240, 60, 3],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42, 241],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42, 241, 98],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42, 241, 98, 1],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42, 241, 98, 1, 126],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42, 241, 98, 1, 126, 6],\n",
              " [238, 4, 5, 239, 240, 60, 3, 42, 241, 98, 1, 126, 6, 40],\n",
              " [127, 6],\n",
              " [127, 6, 1],\n",
              " [127, 6, 1, 31],\n",
              " [127, 6, 1, 31, 14],\n",
              " [127, 6, 1, 31, 14, 5],\n",
              " [127, 6, 1, 31, 14, 5, 8],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13, 242],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13, 242, 3],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13, 242, 3, 42],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13, 242, 3, 42, 12],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13, 242, 3, 42, 12, 96],\n",
              " [127, 6, 1, 31, 14, 5, 8, 86, 35, 13, 242, 3, 42, 12, 96, 6],\n",
              " [14, 5],\n",
              " [14, 5, 62],\n",
              " [14, 5, 62, 9],\n",
              " [14, 5, 62, 9, 1],\n",
              " [14, 5, 62, 9, 1, 243],\n",
              " [14, 5, 62, 9, 1, 243, 244],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27, 4],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27, 4, 13],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27, 4, 13, 245],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27, 4, 13, 245, 21],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27, 4, 13, 245, 21, 57],\n",
              " [14, 5, 62, 9, 1, 243, 244, 27, 4, 13, 245, 21, 57, 39],\n",
              " [128, 129],\n",
              " [128, 129, 2],\n",
              " [128, 129, 2, 16],\n",
              " [128, 129, 2, 16, 62],\n",
              " [128, 129, 2, 16, 62, 1],\n",
              " [128, 129, 2, 16, 62, 1, 246],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6, 27],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6, 27, 4],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6, 27, 4, 13],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6, 27, 4, 13, 130],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6, 27, 4, 13, 130, 129],\n",
              " [128, 129, 2, 16, 62, 1, 246, 6, 27, 4, 13, 130, 129, 247],\n",
              " [131, 9],\n",
              " [131, 9, 10],\n",
              " [131, 9, 10, 60],\n",
              " [131, 9, 10, 60, 12],\n",
              " [131, 9, 10, 60, 12, 112],\n",
              " [131, 9, 10, 60, 12, 112, 2],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24, 248],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24, 248, 1],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24, 248, 1, 69],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24, 248, 1, 69, 6],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24, 248, 1, 69, 6, 27],\n",
              " [131, 9, 10, 60, 12, 112, 2, 4, 10, 24, 248, 1, 69, 6, 27, 19],\n",
              " [3, 108],\n",
              " [3, 108, 1],\n",
              " [3, 108, 1, 249],\n",
              " [3, 108, 1, 249, 33],\n",
              " [3, 108, 1, 249, 33, 4],\n",
              " [3, 108, 1, 249, 33, 4, 2],\n",
              " [3, 108, 1, 249, 33, 4, 2, 20],\n",
              " [113, 72],\n",
              " [113, 72, 25],\n",
              " [113, 72, 25, 68],\n",
              " [45, 46],\n",
              " [45, 46, 7],\n",
              " [45, 46, 7, 48],\n",
              " [115, 46],\n",
              " [14, 5],\n",
              " [14, 5, 48],\n",
              " [14, 5, 48, 22],\n",
              " [14, 5, 48, 22, 38],\n",
              " [49, 16],\n",
              " [49, 16, 48],\n",
              " [49, 16, 48, 49],\n",
              " [49, 16, 48, 49, 2],\n",
              " [49, 16, 48, 49, 2, 16],\n",
              " [4, 5],\n",
              " [4, 5, 14],\n",
              " [4, 5, 14, 5],\n",
              " [4, 5, 14, 5, 49],\n",
              " [4, 5, 14, 5, 49, 16],\n",
              " [7, 16],\n",
              " [7, 16, 25],\n",
              " [7, 16, 25, 250],\n",
              " [114, 14],\n",
              " [114, 14, 5],\n",
              " [114, 14, 5, 4],\n",
              " [114, 14, 5, 4, 5],\n",
              " [114, 14, 5, 4, 5, 21],\n",
              " [114, 14, 5, 4, 5, 21, 8],\n",
              " [114, 14, 5, 4, 5, 21, 8, 251],\n",
              " [114, 14, 5, 4, 5, 21, 8, 251, 49],\n",
              " [114, 14, 5, 4, 5, 21, 8, 251, 49, 16],\n",
              " [7, 26],\n",
              " [7, 26, 15],\n",
              " [7, 26, 15, 25],\n",
              " [7, 26, 15, 25, 15],\n",
              " [7, 26, 15, 25, 15, 14],\n",
              " [7, 26, 15, 25, 15, 14, 5],\n",
              " [7, 26, 15, 25, 15, 14, 5, 14],\n",
              " [7, 26, 15, 25, 15, 14, 5, 14, 5],\n",
              " [132, 63],\n",
              " [132, 63, 253],\n",
              " [132, 63, 253, 254],\n",
              " [45, 47],\n",
              " [45, 47, 7],\n",
              " [45, 47, 7, 22],\n",
              " [45, 47, 7, 22, 29],\n",
              " [50, 133],\n",
              " [50, 133, 255],\n",
              " [50, 133, 255, 134],\n",
              " [50, 133, 255, 134, 22],\n",
              " [50, 133, 255, 134, 22, 29],\n",
              " [50, 133, 255, 134, 22, 29, 21],\n",
              " [50, 133, 255, 134, 22, 29, 21, 134],\n",
              " [50, 133, 255, 134, 22, 29, 21, 134, 8],\n",
              " [50, 133, 255, 134, 22, 29, 21, 134, 8, 7],\n",
              " [50, 133, 255, 134, 22, 29, 21, 134, 8, 7, 16],\n",
              " [50, 133, 255, 134, 22, 29, 21, 134, 8, 7, 16, 5],\n",
              " [50, 133, 255, 134, 22, 29, 21, 134, 8, 7, 16, 5, 51],\n",
              " [50, 7],\n",
              " [50, 7, 26],\n",
              " [50, 7, 26, 15],\n",
              " [50, 7, 26, 15, 50],\n",
              " [70, 50],\n",
              " [256, 9],\n",
              " [256, 9, 1],\n",
              " [256, 9, 1, 257],\n",
              " [256, 9, 1, 257, 26],\n",
              " [256, 9, 1, 257, 26, 18],\n",
              " [256, 9, 1, 257, 26, 18, 1],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2, 16],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2, 16, 13],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2, 16, 13, 95],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2, 16, 13, 95, 258],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2, 16, 13, 95, 258, 259],\n",
              " [256, 9, 1, 257, 26, 18, 1, 2, 16, 13, 95, 258, 259, 12],\n",
              " [260, 15],\n",
              " [260, 15, 20],\n",
              " [260, 15, 20, 3],\n",
              " [260, 15, 20, 3, 121],\n",
              " [260, 15, 20, 3, 121, 135],\n",
              " [260, 15, 20, 3, 121, 135, 26],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261, 6],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261, 6, 71],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261, 6, 71, 123],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261, 6, 71, 123, 38],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261, 6, 71, 123, 38, 124],\n",
              " [260, 15, 20, 3, 121, 135, 26, 261, 6, 71, 123, 38, 124, 125],\n",
              " [22, 5],\n",
              " [22, 5, 9],\n",
              " [22, 5, 9, 13],\n",
              " [22, 5, 9, 13, 262],\n",
              " [22, 5, 9, 13, 262, 21],\n",
              " [22, 5, 9, 13, 262, 21, 1],\n",
              " [22, 5, 9, 13, 262, 21, 1, 136],\n",
              " [22, 5, 9, 13, 262, 21, 1, 136, 47],\n",
              " [22, 5, 9, 13, 262, 21, 1, 136, 47, 263],\n",
              " [22, 5, 9, 13, 262, 21, 1, 136, 47, 263, 264],\n",
              " [22, 5, 9, 13, 262, 21, 1, 136, 47, 263, 264, 3],\n",
              " [22, 5, 9, 13, 262, 21, 1, 136, 47, 263, 264, 3, 265],\n",
              " [44, 43],\n",
              " [44, 43, 105],\n",
              " [44, 43, 105, 1],\n",
              " [44, 43, 105, 1, 33],\n",
              " [44, 43, 105, 1, 33, 4],\n",
              " [44, 43, 105, 1, 33, 4, 2],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266, 1],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266, 1, 267],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266, 1, 267, 71],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266, 1, 267, 71, 6],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266, 1, 267, 71, 6, 83],\n",
              " [44, 43, 105, 1, 33, 4, 2, 20, 266, 1, 267, 71, 6, 83, 32],\n",
              " [10, 268],\n",
              " [10, 268, 1],\n",
              " [10, 268, 1, 269],\n",
              " [10, 268, 1, 269, 10],\n",
              " [10, 268, 1, 269, 10, 270],\n",
              " [10, 268, 1, 269, 10, 270, 271],\n",
              " [10, 268, 1, 269, 10, 270, 271, 18],\n",
              " [10, 268, 1, 269, 10, 270, 271, 18, 1],\n",
              " [10, 268, 1, 269, 10, 270, 271, 18, 1, 73],\n",
              " [10, 268, 1, 269, 10, 270, 271, 18, 1, 73, 39],\n",
              " [10, 268, 1, 269, 10, 270, 271, 18, 1, 73, 39, 23],\n",
              " [10, 268, 1, 269, 10, 270, 271, 18, 1, 73, 39, 23, 272],\n",
              " [273, 1],\n",
              " [273, 1, 72],\n",
              " [273, 1, 72, 68],\n",
              " [273, 1, 72, 68, 37],\n",
              " [273, 1, 72, 68, 37, 274],\n",
              " [273, 1, 72, 68, 37, 274, 9],\n",
              " [273, 1, 72, 68, 37, 274, 9, 10],\n",
              " [273, 1, 72, 68, 37, 274, 9, 10, 52],\n",
              " [273, 1, 72, 68, 37, 274, 9, 10, 52, 1],\n",
              " [273, 1, 72, 68, 37, 274, 9, 10, 52, 1, 74],\n",
              " [273, 1, 72, 68, 37, 274, 9, 10, 52, 1, 74, 91],\n",
              " [275, 53],\n",
              " [275, 53, 39],\n",
              " [137, 30],\n",
              " [137, 30, 72],\n",
              " [137, 30, 72, 48],\n",
              " [30, 26],\n",
              " [30, 26, 137],\n",
              " [30, 26, 137, 30],\n",
              " [30, 26, 137, 30, 32],\n",
              " [30, 26, 137, 30, 32, 276],\n",
              " [30, 26],\n",
              " [30, 26, 38],\n",
              " [133, 38],\n",
              " [133, 38, 51],\n",
              " [133, 38, 51, 64],\n",
              " [133, 38, 51, 64, 130],\n",
              " [35, 277],\n",
              " [35, 277, 3],\n",
              " [35, 277, 3, 278],\n",
              " [35, 277, 3, 278, 111],\n",
              " [35, 277, 3, 278, 111, 37],\n",
              " [35, 277, 3, 278, 111, 37, 279],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44, 52],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44, 52, 75],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44, 52, 75, 280],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44, 52, 75, 280, 3],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44, 52, 75, 280, 3, 281],\n",
              " [35, 277, 3, 278, 111, 37, 279, 28, 2, 44, 52, 75, 280, 3, 281, 1],\n",
              " [2, 21],\n",
              " [2, 21, 138],\n",
              " [2, 21, 138, 93],\n",
              " [2, 21, 138, 93, 282],\n",
              " [2, 21, 138, 93, 282, 6],\n",
              " [2, 21, 138, 93, 282, 6, 1],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119, 10],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119, 10, 24],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119, 10, 24, 52],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119, 10, 24, 52, 1],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119, 10, 24, 52, 1, 4],\n",
              " [2, 21, 138, 93, 282, 6, 1, 76, 77, 119, 10, 24, 52, 1, 4, 78],\n",
              " [283, 18],\n",
              " [283, 18, 75],\n",
              " [283, 18, 75, 109],\n",
              " [283, 18, 75, 109, 284],\n",
              " [283, 18, 75, 109, 284, 1],\n",
              " [283, 18, 75, 109, 284, 1, 79],\n",
              " [283, 18, 75, 109, 284, 1, 79, 6],\n",
              " [283, 18, 75, 109, 284, 1, 79, 6, 12],\n",
              " [283, 18, 75, 109, 284, 1, 79, 6, 12, 73],\n",
              " [283, 18, 75, 109, 284, 1, 79, 6, 12, 73, 285],\n",
              " [283, 18, 75, 109, 284, 1, 79, 6, 12, 73, 285, 11],\n",
              " [286, 66],\n",
              " [286, 66, 1],\n",
              " [286, 66, 1, 17],\n",
              " [286, 66, 1, 17, 287],\n",
              " [286, 66, 1, 17, 287, 288],\n",
              " [18, 75],\n",
              " [18, 75, 139],\n",
              " [18, 75, 139, 4],\n",
              " [18, 75, 139, 4, 78],\n",
              " [18, 289],\n",
              " [18, 289, 139],\n",
              " [18, 289, 139, 140],\n",
              " [23, 140],\n",
              " [23, 140, 18],\n",
              " [23, 140, 18, 73],\n",
              " [23, 140, 18, 73, 23],\n",
              " [23, 140, 18, 73, 23, 290],\n",
              " [23, 140, 18, 73, 23, 290, 26],\n",
              " [23, 140, 18, 73, 23, 290, 26, 79],\n",
              " [23, 140, 18, 73, 23, 290, 26, 79, 291],\n",
              " [17, 12],\n",
              " [17, 12, 292],\n",
              " [17, 12, 292, 141],\n",
              " [17, 12, 292, 141, 142],\n",
              " [17, 12, 292, 141, 142, 135],\n",
              " [17, 12, 292, 141, 142, 135, 293],\n",
              " [17, 54],\n",
              " [17, 54, 294],\n",
              " [17, 54, 294, 141],\n",
              " [17, 54, 294, 141, 142],\n",
              " [17, 54, 294, 141, 142, 12],\n",
              " [17, 54, 294, 141, 142, 12, 295],\n",
              " [55, 32],\n",
              " [55, 32, 80],\n",
              " [55, 32, 80, 17],\n",
              " [55, 32, 80, 17, 12],\n",
              " [55, 32, 80, 17, 12, 17],\n",
              " [55, 32, 80, 17, 12, 17, 54],\n",
              " [55, 32, 80, 17, 12, 17, 54, 70],\n",
              " [55, 32, 80, 17, 12, 17, 54, 70, 122],\n",
              " [55, 32, 80, 17, 12, 17, 54, 70, 122, 296],\n",
              " [2, 23],\n",
              " [2, 23, 55],\n",
              " [2, 23, 55, 32],\n",
              " [2, 23, 55, 32, 79],\n",
              " [17, 54],\n",
              " [17, 54, 143],\n",
              " [17, 54, 143, 55],\n",
              " [17, 54, 143, 55, 32],\n",
              " [17, 54, 143, 55, 32, 144],\n",
              " [17, 54, 143, 55, 32, 144, 297],\n",
              " [17, 54, 143, 55, 32, 144, 297, 81],\n",
              " [17, 54, 143, 55, 32, 144, 297, 81, 82],\n",
              " [17, 54, 143, 55, 32, 144, 297, 81, 82, 298],\n",
              " [17, 54, 143, 55, 32, 144, 297, 81, 82, 298, 5],\n",
              " [17, 54, 143, 55, 32, 144, 297, 81, 82, 298, 5, 51],\n",
              " [31, 80],\n",
              " [31, 80, 299],\n",
              " [31, 80, 299, 81],\n",
              " [31, 80, 299, 81, 3],\n",
              " [31, 80, 299, 81, 3, 31],\n",
              " [31, 80, 299, 81, 3, 31, 55],\n",
              " [31, 80, 299, 81, 3, 31, 55, 32],\n",
              " [31, 80, 299, 81, 3, 31, 55, 32, 145],\n",
              " [31, 80, 299, 81, 3, 31, 55, 32, 145, 81],\n",
              " [31, 80, 299, 81, 3, 31, 55, 32, 145, 81, 82],\n",
              " [4, 78],\n",
              " [4, 78, 2],\n",
              " [4, 78, 2, 31],\n",
              " [4, 78, 2, 31, 17],\n",
              " [4, 78, 2, 31, 17, 54],\n",
              " [4, 78, 2, 31, 17, 54, 143],\n",
              " [4, 78, 2, 31, 17, 54, 143, 16],\n",
              " [4, 78, 2, 31, 17, 54, 143, 16, 300],\n",
              " [1, 67],\n",
              " [37, 146],\n",
              " [37, 146, 302],\n",
              " [37, 146, 302, 1],\n",
              " [37, 146, 302, 1, 2],\n",
              " [37, 146, 302, 1, 2, 34],\n",
              " [37, 146, 302, 1, 2, 34, 53],\n",
              " [37, 146, 302, 1, 2, 34, 53, 56],\n",
              " [37, 146, 302, 1, 2, 34, 53, 56, 303],\n",
              " [37, 146, 302, 1, 2, 34, 53, 56, 303, 1],\n",
              " [37, 146, 302, 1, 2, 34, 53, 56, 303, 1, 144],\n",
              " [37, 146, 302, 1, 2, 34, 53, 56, 303, 1, 144, 304],\n",
              " [40, 13],\n",
              " [40, 13, 305],\n",
              " [40, 13, 305, 306],\n",
              " [40, 13, 305, 306, 307],\n",
              " [40, 13, 305, 306, 307, 65],\n",
              " [40, 13, 305, 306, 307, 65, 308],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76, 9],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76, 9, 13],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76, 9, 13, 309],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76, 9, 13, 309, 310],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76, 9, 13, 309, 310, 3],\n",
              " [40, 13, 305, 306, 307, 65, 308, 76, 9, 13, 309, 310, 3, 311],\n",
              " [1, 312],\n",
              " [1, 312, 6],\n",
              " [1, 312, 6, 1],\n",
              " [1, 312, 6, 1, 56],\n",
              " [1, 312, 6, 1, 56, 313],\n",
              " [1, 312, 6, 1, 56, 313, 1],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314, 6],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314, 6, 1],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314, 6, 1, 2],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314, 6, 1, 2, 34],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314, 6, 1, 2, 34, 65],\n",
              " [1, 312, 6, 1, 56, 313, 1, 314, 6, 1, 2, 34, 65, 315],\n",
              " [56, 61],\n",
              " [56, 61, 147],\n",
              " [56, 61, 147, 148],\n",
              " [56, 61, 147, 148, 3],\n",
              " [56, 61, 147, 148, 3, 51],\n",
              " [56, 61, 147, 148, 3, 51, 11],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316, 56],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316, 56, 61],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316, 56, 61, 147],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316, 56, 61, 147, 148],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316, 56, 61, 147, 148, 3],\n",
              " [56, 61, 147, 148, 3, 51, 11, 316, 56, 61, 147, 148, 3, 82],\n",
              " [8, 37],\n",
              " [8, 37, 57],\n",
              " [8, 37, 57, 1],\n",
              " [8, 37, 57, 1, 145],\n",
              " [8, 37, 57, 1, 145, 317],\n",
              " [8, 37, 57, 1, 145, 317, 6],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318, 11],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318, 11, 1],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318, 11, 1, 319],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318, 11, 1, 319, 11],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318, 11, 1, 319, 11, 320],\n",
              " [8, 37, 57, 1, 145, 317, 6, 138, 318, 11, 1, 319, 11, 320, 31],\n",
              " [36, 1],\n",
              " [36, 1, 321],\n",
              " [36, 1, 321, 31],\n",
              " [36, 1, 321, 31, 8],\n",
              " [36, 1, 321, 31, 8, 322],\n",
              " [36, 1, 321, 31, 8, 322, 80],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9, 10],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9, 10, 323],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9, 10, 323, 8],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9, 10, 323, 8, 132],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9, 10, 323, 8, 132, 324],\n",
              " [36, 1, 321, 31, 8, 322, 80, 9, 10, 323, 8, 132, 324, 19],\n",
              " [325, 10],\n",
              " [325, 10, 24],\n",
              " [325, 10, 24, 43],\n",
              " [325, 10, 24, 43, 18],\n",
              " [325, 10, 24, 43, 18, 1],\n",
              " [325, 10, 24, 43, 18, 1, 146],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13, 9],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13, 9, 1],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13, 9, 1, 2],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13, 9, 1, 2, 34],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13, 9, 1, 2, 34, 36],\n",
              " [325, 10, 24, 43, 18, 1, 146, 13, 9, 1, 2, 34, 36, 326],\n",
              " [327, 149],\n",
              " [327, 149, 9],\n",
              " [327, 149, 9, 328],\n",
              " [327, 149, 9, 328, 3],\n",
              " [327, 149, 9, 328, 3, 1],\n",
              " [327, 149, 9, 328, 3, 1, 74],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59, 329],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59, 329, 39],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59, 329, 39, 24],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59, 329, 39, 24, 330],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59, 329, 39, 24, 330, 9],\n",
              " [327, 149, 9, 328, 3, 1, 74, 17, 59, 329, 39, 24, 330, 9, 35],\n",
              " [331, 332],\n",
              " [331, 332, 3],\n",
              " [331, 332, 3, 149],\n",
              " [331, 332, 3, 149, 8],\n",
              " [331, 332, 3, 149, 8, 1],\n",
              " [331, 332, 3, 149, 8, 1, 74],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21, 1],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21, 1, 76],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21, 1, 76, 77],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21, 1, 76, 77, 10],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21, 1, 76, 77, 10, 24],\n",
              " [331, 332, 3, 149, 8, 1, 74, 17, 333, 21, 1, 76, 77, 10, 24, 43],\n",
              " [9, 39],\n",
              " [9, 39, 128],\n",
              " [9, 39, 128, 334],\n",
              " [9, 39, 128, 334, 335],\n",
              " [9, 39, 128, 334, 335, 53],\n",
              " [9, 39, 128, 334, 335, 53, 336],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8, 1],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8, 1, 110],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8, 1, 110, 17],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8, 1, 110, 17, 11],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8, 1, 110, 17, 11, 337],\n",
              " [9, 39, 128, 334, 335, 53, 336, 8, 1, 110, 17, 11, 337, 11],\n",
              " [338, 8],\n",
              " [338, 8, 1],\n",
              " [338, 8, 1, 339],\n",
              " [338, 8, 1, 339, 58],\n",
              " [338, 8, 1, 339, 58, 2],\n",
              " [338, 8, 1, 339, 58, 2, 34],\n",
              " [338, 8, 1, 339, 58, 2, 34, 340],\n",
              " [338, 8, 1, 339, 58, 2, 34, 340, 1],\n",
              " [338, 8, 1, 339, 58, 2, 34, 340, 1, 23],\n",
              " [338, 8, 1, 339, 58, 2, 34, 340, 1, 23, 3],\n",
              " [338, 8, 1, 339, 58, 2, 34, 340, 1, 23, 3, 341],\n",
              " [338, 8, 1, 339, 58, 2, 34, 340, 1, 23, 3, 341, 1],\n",
              " [52, 6],\n",
              " [52, 6, 77],\n",
              " [52, 6, 77, 53],\n",
              " [52, 6, 77, 53, 12],\n",
              " [52, 6, 77, 53, 12, 101],\n",
              " [52, 6, 77, 53, 12, 101, 342],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28, 1],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28, 1, 345],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28, 1, 345, 8],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28, 1, 345, 8, 59],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28, 1, 345, 8, 59, 35],\n",
              " [52, 6, 77, 53, 12, 101, 342, 343, 344, 28, 1, 345, 8, 59, 35, 346],\n",
              " [131, 9],\n",
              " [131, 9, 120],\n",
              " [131, 9, 120, 347],\n",
              " [131, 9, 120, 347, 2],\n",
              " [131, 9, 120, 347, 2, 44],\n",
              " [131, 9, 120, 347, 2, 44, 348],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12, 349],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12, 349, 100],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12, 349, 100, 350],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12, 349, 100, 350, 1],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12, 349, 100, 350, 1, 351],\n",
              " [131, 9, 120, 347, 2, 44, 348, 12, 349, 100, 350, 1, 351, 352],\n",
              " [6, 1],\n",
              " [6, 1, 67],\n",
              " [6, 1, 67, 20],\n",
              " [6, 1, 67, 20, 353],\n",
              " [6, 1, 67, 20, 353, 354],\n",
              " [6, 1, 67, 20, 353, 354, 136],\n",
              " [6, 1, 67, 20, 353, 354, 136, 47],\n",
              " [6, 1, 67, 20, 353, 354, 136, 47, 107]]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CrzbvUUQCXPU"
      },
      "outputs": [],
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9oPMoWBSD1_U"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miRb-QZyIi7_",
        "outputId": "de5c2b4e-b6e6-4e4b-a892-f97eff72c1c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  33, 150],\n",
              "       [  0,   0,   0, ...,  33, 150,   2],\n",
              "       [  0,   0,   0, ...,   0,   8,  83],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 353, 354, 136],\n",
              "       [  0,   0,   0, ..., 354, 136,  47],\n",
              "       [  0,   0,   0, ..., 136,  47, 107]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qVI0-UUrIsd3"
      },
      "outputs": [],
      "source": [
        "X = padded_input_sequences[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lXrYHTDFI3uE"
      },
      "outputs": [],
      "source": [
        "y = padded_input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmsFnHx1Qdow",
        "outputId": "38cc3392-f099-49e2-f0c9-52955380d43e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(840, 16)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_length_v = X.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wyYqYgZSeck",
        "outputId": "5c223cc0-a8e9-4c4f-a364-a436bc492abf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(840,)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5OL3vrEXSs_s"
      },
      "outputs": [],
      "source": [
        "v=len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rs1NPitwSgzk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=v+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQMJ0I6xSiZf",
        "outputId": "bba7b827-0696-427e-9d3d-c811600ce5a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(840, 355)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9kVeTvR2S8Fk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wo-OYfHpTK2o"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(v+1, 100, input_length=input_length_v))\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(v+1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-GGjqh7ue_Yq"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxxXkrSXfIBv",
        "outputId": "d809aa2b-600f-4549-cfc1-1dfd1fef1a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 16, 100)           35500     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 16, 150)           150600    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 150)               180600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 355)               53605     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 420,305\n",
            "Trainable params: 420,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpFUCALCfJRR",
        "outputId": "96d67a78-3c2e-4462-b2a8-2655c303af8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "27/27 [==============================] - 6s 31ms/step - loss: 5.7076 - accuracy: 0.0417\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 1s 35ms/step - loss: 5.3494 - accuracy: 0.0667\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 5.2662 - accuracy: 0.0667\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 1s 35ms/step - loss: 5.2430 - accuracy: 0.0667\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 5.2004 - accuracy: 0.0655\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 1s 39ms/step - loss: 5.1328 - accuracy: 0.0726\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 1s 55ms/step - loss: 5.0398 - accuracy: 0.0810\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 2s 56ms/step - loss: 4.9311 - accuracy: 0.0821\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 2s 62ms/step - loss: 4.8451 - accuracy: 0.0833\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 2s 61ms/step - loss: 4.7497 - accuracy: 0.0905\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 2s 61ms/step - loss: 4.6618 - accuracy: 0.0917\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 2s 55ms/step - loss: 4.5572 - accuracy: 0.1012\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 4.4597 - accuracy: 0.1060\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 1s 44ms/step - loss: 4.3541 - accuracy: 0.1095\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 2s 61ms/step - loss: 4.2586 - accuracy: 0.1143\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 1s 53ms/step - loss: 4.1559 - accuracy: 0.1083\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 1s 51ms/step - loss: 4.0722 - accuracy: 0.1167\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 2s 60ms/step - loss: 3.9570 - accuracy: 0.1310\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 2s 62ms/step - loss: 3.8782 - accuracy: 0.1405\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 2s 62ms/step - loss: 3.7770 - accuracy: 0.1464\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 2s 62ms/step - loss: 3.6797 - accuracy: 0.1607\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 3.5664 - accuracy: 0.1655\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 2s 71ms/step - loss: 3.4748 - accuracy: 0.1750\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 3.3974 - accuracy: 0.1869\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 3.3029 - accuracy: 0.2167\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 3.2119 - accuracy: 0.2238\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 2s 73ms/step - loss: 3.0958 - accuracy: 0.2655\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 2s 69ms/step - loss: 3.0086 - accuracy: 0.2917\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 2s 54ms/step - loss: 2.9084 - accuracy: 0.2976\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 2.8237 - accuracy: 0.3095\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 2s 78ms/step - loss: 2.7598 - accuracy: 0.3464\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 2s 80ms/step - loss: 2.6498 - accuracy: 0.3714\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 2.5488 - accuracy: 0.3869\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 2s 80ms/step - loss: 2.4701 - accuracy: 0.4274\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 2s 81ms/step - loss: 2.3752 - accuracy: 0.4726\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 2.2899 - accuracy: 0.4833\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 2s 73ms/step - loss: 2.2111 - accuracy: 0.5202\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 2s 79ms/step - loss: 2.1470 - accuracy: 0.5333\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 2s 77ms/step - loss: 2.0528 - accuracy: 0.5536\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 1.9901 - accuracy: 0.5857\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.9173 - accuracy: 0.5940\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 1.8529 - accuracy: 0.6202\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 2s 72ms/step - loss: 1.8021 - accuracy: 0.6143\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 1.7149 - accuracy: 0.6476\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 1.6631 - accuracy: 0.6631\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 1.6130 - accuracy: 0.6702\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 2s 60ms/step - loss: 1.5323 - accuracy: 0.7190\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.4653 - accuracy: 0.7345\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 2s 56ms/step - loss: 1.4043 - accuracy: 0.7417\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 2s 56ms/step - loss: 1.3565 - accuracy: 0.7571\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 1.3085 - accuracy: 0.7714\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 1.2679 - accuracy: 0.7869\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 1.2191 - accuracy: 0.7929\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 1.1723 - accuracy: 0.8036\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 1.1252 - accuracy: 0.8226\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 1.0821 - accuracy: 0.8226\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 1.0270 - accuracy: 0.8357\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 2s 66ms/step - loss: 0.9970 - accuracy: 0.8405\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 0.9518 - accuracy: 0.8524\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 0.9227 - accuracy: 0.8583\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 2s 73ms/step - loss: 0.8853 - accuracy: 0.8619\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 2s 66ms/step - loss: 0.8525 - accuracy: 0.8655\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 2s 60ms/step - loss: 0.8218 - accuracy: 0.8726\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 1s 48ms/step - loss: 0.7884 - accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.7688 - accuracy: 0.8857\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.7389 - accuracy: 0.8917\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 1s 40ms/step - loss: 0.7111 - accuracy: 0.8786\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 1s 42ms/step - loss: 0.6837 - accuracy: 0.8940\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 2s 72ms/step - loss: 0.6700 - accuracy: 0.9012\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 2s 69ms/step - loss: 0.6430 - accuracy: 0.9012\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 0.6117 - accuracy: 0.9119\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 2s 72ms/step - loss: 0.5892 - accuracy: 0.9131\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 2s 71ms/step - loss: 0.5729 - accuracy: 0.9226\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 0.5526 - accuracy: 0.9214\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 2s 58ms/step - loss: 0.5345 - accuracy: 0.9190\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 2s 73ms/step - loss: 0.5111 - accuracy: 0.9286\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 2s 58ms/step - loss: 0.4994 - accuracy: 0.9357\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 0.4816 - accuracy: 0.9274\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 0.4717 - accuracy: 0.9333\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 2s 69ms/step - loss: 0.4517 - accuracy: 0.9369\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 2s 66ms/step - loss: 0.4353 - accuracy: 0.9321\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 2s 69ms/step - loss: 0.4209 - accuracy: 0.9369\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 0.4103 - accuracy: 0.9393\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 0.3952 - accuracy: 0.9452\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 0.3855 - accuracy: 0.9452\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 2s 56ms/step - loss: 0.3723 - accuracy: 0.9464\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 1s 35ms/step - loss: 0.3679 - accuracy: 0.9393\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 1s 52ms/step - loss: 0.3555 - accuracy: 0.9429\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.3484 - accuracy: 0.9405\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 1s 46ms/step - loss: 0.3308 - accuracy: 0.9536\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 2s 57ms/step - loss: 0.3275 - accuracy: 0.9500\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 1s 52ms/step - loss: 0.3171 - accuracy: 0.9464\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 2s 71ms/step - loss: 0.3141 - accuracy: 0.9500\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 0.3006 - accuracy: 0.9524\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 0.2908 - accuracy: 0.9476\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 0.2834 - accuracy: 0.9536\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 2s 73ms/step - loss: 0.2746 - accuracy: 0.9524\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 0.2697 - accuracy: 0.9536\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 2s 71ms/step - loss: 0.2593 - accuracy: 0.9500\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 2s 58ms/step - loss: 0.2573 - accuracy: 0.9488\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2d51e51ddf0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X,y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGeYGwCMfTus",
        "outputId": "2d508555-b83e-470e-e7e5-1b5c10cce70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 806ms/step\n",
            "In our\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "In our simple\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "In our simple example\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "In our simple example we\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "In our simple example we only\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "In our simple example we only used\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "In our simple example we only used the\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "In our simple example we only used the embeddings\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "In our simple example we only used the embeddings “as\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "In our simple example we only used the embeddings “as is”\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "text = \"In\"\n",
        "import numpy as np\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=input_length_v, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTxsj-_CjbQW",
        "outputId": "7e8e0b43-d8f5-4e87-8caf-33806965ab3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'attention': 2,\n",
              " 'to': 3,\n",
              " 'head': 4,\n",
              " 'dim': 5,\n",
              " 'of': 6,\n",
              " 'self': 7,\n",
              " 'in': 8,\n",
              " 'that': 9,\n",
              " 'we': 10,\n",
              " 'and': 11,\n",
              " 'a': 12,\n",
              " 'is': 13,\n",
              " 'embed': 14,\n",
              " 'linear': 15,\n",
              " 'heads': 16,\n",
              " 'sentence': 17,\n",
              " 'from': 18,\n",
              " 'one': 19,\n",
              " 'layer': 20,\n",
              " 'for': 21,\n",
              " 'hidden': 22,\n",
              " 'model': 23,\n",
              " 'can': 24,\n",
              " 'nn': 25,\n",
              " 'output': 26,\n",
              " 'each': 27,\n",
              " 'on': 28,\n",
              " 'state': 29,\n",
              " 'attn': 30,\n",
              " 'tokens': 31,\n",
              " 'inputs': 32,\n",
              " 'multi': 33,\n",
              " 'weights': 34,\n",
              " 'it': 35,\n",
              " 'are': 36,\n",
              " 'this': 37,\n",
              " 'size': 38,\n",
              " 'bert': 39,\n",
              " 'embedding': 40,\n",
              " 'focus': 41,\n",
              " 'be': 42,\n",
              " 'see': 43,\n",
              " 'let’s': 44,\n",
              " 'def': 45,\n",
              " 'init': 46,\n",
              " 'forward': 47,\n",
              " 'config': 48,\n",
              " 'num': 49,\n",
              " 'x': 50,\n",
              " '1': 51,\n",
              " 'use': 52,\n",
              " 'as': 53,\n",
              " 'b': 54,\n",
              " 'viz': 55,\n",
              " 'lines': 56,\n",
              " 'example': 57,\n",
              " 'these': 58,\n",
              " 'which': 59,\n",
              " 'have': 60,\n",
              " 'representing': 61,\n",
              " 'so': 62,\n",
              " '3': 63,\n",
              " '5': 64,\n",
              " 'with': 65,\n",
              " 'where': 66,\n",
              " 'encoder': 67,\n",
              " 'module': 68,\n",
              " 'outputs': 69,\n",
              " 'return': 70,\n",
              " 'shape': 71,\n",
              " 'multiheadattention': 72,\n",
              " 'pretrained': 73,\n",
              " 'same': 74,\n",
              " 'bertviz': 75,\n",
              " 'word': 76,\n",
              " '“flies”': 77,\n",
              " 'view': 78,\n",
              " 'attentions': 79,\n",
              " 'tokenizer': 80,\n",
              " 'ids': 81,\n",
              " '0': 82,\n",
              " 'our': 83,\n",
              " 'embeddings': 84,\n",
              " 'but': 85,\n",
              " 'practice': 86,\n",
              " 'three': 87,\n",
              " 'independent': 88,\n",
              " 'transformations': 89,\n",
              " 'vectors': 90,\n",
              " 'set': 91,\n",
              " 'allows': 92,\n",
              " 'different': 93,\n",
              " 'aspects': 94,\n",
              " 'also': 95,\n",
              " 'multiple': 96,\n",
              " 'figure': 97,\n",
              " 'than': 98,\n",
              " 'several': 99,\n",
              " 'at': 100,\n",
              " 'verb': 101,\n",
              " 'another': 102,\n",
              " 'finds': 103,\n",
              " 'into': 104,\n",
              " 'if': 105,\n",
              " 'you': 106,\n",
              " 'networks': 107,\n",
              " 'implement': 108,\n",
              " 'by': 109,\n",
              " 'first': 110,\n",
              " 'up': 111,\n",
              " 'single': 112,\n",
              " 'class': 113,\n",
              " 'attentionhead': 114,\n",
              " 'super': 115,\n",
              " 'q': 116,\n",
              " 'k': 117,\n",
              " 'v': 118,\n",
              " 'here': 119,\n",
              " 'we’ve': 120,\n",
              " 'produce': 121,\n",
              " 'tensors': 122,\n",
              " 'batch': 123,\n",
              " 'seq': 124,\n",
              " 'len': 125,\n",
              " 'number': 126,\n",
              " 'dimensions': 127,\n",
              " 'has': 128,\n",
              " '12': 129,\n",
              " '768': 130,\n",
              " 'now': 131,\n",
              " 'chapter': 132,\n",
              " 'torch': 133,\n",
              " 'h': 134,\n",
              " 'an': 135,\n",
              " 'feed': 136,\n",
              " 'multihead': 137,\n",
              " 'two': 138,\n",
              " 'import': 139,\n",
              " 'automodel': 140,\n",
              " 'flies': 141,\n",
              " 'like': 142,\n",
              " 'start': 143,\n",
              " 'token': 144,\n",
              " 'input': 145,\n",
              " 'visualization': 146,\n",
              " 'values': 147,\n",
              " 'close': 148,\n",
              " 'words': 149,\n",
              " 'headed': 150,\n",
              " 'simple': 151,\n",
              " 'only': 152,\n",
              " 'used': 153,\n",
              " '“as': 154,\n",
              " 'is”': 155,\n",
              " 'compute': 156,\n",
              " 'scores': 157,\n",
              " 'that’s': 158,\n",
              " 'far': 159,\n",
              " 'whole': 160,\n",
              " 'story': 161,\n",
              " 'applies': 162,\n",
              " 'generate': 163,\n",
              " 'query': 164,\n",
              " 'key': 165,\n",
              " 'value': 166,\n",
              " 'project': 167,\n",
              " 'projection': 168,\n",
              " 'carries': 169,\n",
              " 'its': 170,\n",
              " 'own': 171,\n",
              " 'learnable': 172,\n",
              " 'parameters': 173,\n",
              " 'semantic': 174,\n",
              " 'sequence': 175,\n",
              " 'turns': 176,\n",
              " 'out': 177,\n",
              " 'beneficial': 178,\n",
              " 'sets': 179,\n",
              " 'projections': 180,\n",
              " 'called': 181,\n",
              " 'resulting': 182,\n",
              " 'illustrated': 183,\n",
              " 'why': 184,\n",
              " 'do': 185,\n",
              " 'need': 186,\n",
              " 'more': 187,\n",
              " 'rea': 188,\n",
              " 'son': 189,\n",
              " 'softmax': 190,\n",
              " 'tends': 191,\n",
              " 'mostly': 192,\n",
              " 'aspect': 193,\n",
              " 'similarity': 194,\n",
              " 'having': 195,\n",
              " 'once': 196,\n",
              " 'instance': 197,\n",
              " 'subject': 198,\n",
              " 'interaction': 199,\n",
              " 'whereas': 200,\n",
              " 'nearby': 201,\n",
              " 'adjectives': 202,\n",
              " 'obviously': 203,\n",
              " 'don’t': 204,\n",
              " 'handcraft': 205,\n",
              " 'relations': 206,\n",
              " 'they': 207,\n",
              " 'fully': 208,\n",
              " 'learned': 209,\n",
              " 'data': 210,\n",
              " 'familiar': 211,\n",
              " 'computer': 212,\n",
              " 'vision': 213,\n",
              " 'models': 214,\n",
              " 'might': 215,\n",
              " 'resemblance': 216,\n",
              " 'filters': 217,\n",
              " 'convolutional': 218,\n",
              " 'neural': 219,\n",
              " 'filter': 220,\n",
              " 'responsible': 221,\n",
              " 'detecting': 222,\n",
              " 'faces': 223,\n",
              " 'wheels': 224,\n",
              " 'cars': 225,\n",
              " 'images': 226,\n",
              " '67': 227,\n",
              " 'coding': 228,\n",
              " 'scaled': 229,\n",
              " 'dot': 230,\n",
              " 'product': 231,\n",
              " 'initialized': 232,\n",
              " 'layers': 233,\n",
              " 'apply': 234,\n",
              " 'matrix': 235,\n",
              " 'multiplication': 236,\n",
              " 'projecting': 237,\n",
              " 'although': 238,\n",
              " 'does': 239,\n",
              " 'not': 240,\n",
              " 'smaller': 241,\n",
              " 'chosen': 242,\n",
              " 'computation': 243,\n",
              " 'across': 244,\n",
              " 'constant': 245,\n",
              " 'dimension': 246,\n",
              " '64': 247,\n",
              " 'concatenate': 248,\n",
              " 'full': 249,\n",
              " 'modulelist': 250,\n",
              " 'range': 251,\n",
              " '68': 252,\n",
              " 'transformer': 253,\n",
              " 'anatomy': 254,\n",
              " 'cat': 255,\n",
              " 'notice': 256,\n",
              " 'concatenated': 257,\n",
              " 'fed': 258,\n",
              " 'through': 259,\n",
              " 'final': 260,\n",
              " 'tensor': 261,\n",
              " 'suitable': 262,\n",
              " 'network': 263,\n",
              " 'downstream': 264,\n",
              " 'confirm': 265,\n",
              " 'produces': 266,\n",
              " 'expected': 267,\n",
              " 'pass': 268,\n",
              " 'configuration': 269,\n",
              " 'loaded': 270,\n",
              " 'earlier': 271,\n",
              " 'when': 272,\n",
              " 'initializing': 273,\n",
              " 'ensures': 274,\n",
              " 'tings': 275,\n",
              " 'embeds': 276,\n",
              " 'works': 277,\n",
              " 'wrap': 278,\n",
              " 'section': 279,\n",
              " 'again': 280,\n",
              " 'visualize': 281,\n",
              " 'uses': 282,\n",
              " 'function': 283,\n",
              " 'computing': 284,\n",
              " 'checkpoint': 285,\n",
              " 'indicating': 286,\n",
              " 'boundary': 287,\n",
              " 'lies': 288,\n",
              " 'transformers': 289,\n",
              " 'ckpt': 290,\n",
              " 'true': 291,\n",
              " 'time': 292,\n",
              " 'arrow': 293,\n",
              " 'fruit': 294,\n",
              " 'banana': 295,\n",
              " \"'pt'\": 296,\n",
              " 'type': 297,\n",
              " 'sum': 298,\n",
              " 'convert': 299,\n",
              " '8': 300,\n",
              " '69': 301,\n",
              " 'shows': 302,\n",
              " 'connecting': 303,\n",
              " 'whose': 304,\n",
              " 'getting': 305,\n",
              " 'updated': 306,\n",
              " 'left': 307,\n",
              " 'every': 308,\n",
              " 'being': 309,\n",
              " 'attended': 310,\n",
              " 'right': 311,\n",
              " 'intensity': 312,\n",
              " 'indicates': 313,\n",
              " 'strength': 314,\n",
              " 'dark': 315,\n",
              " 'faint': 316,\n",
              " 'consists': 317,\n",
              " 'sentences': 318,\n",
              " 'cls': 319,\n",
              " 'sep': 320,\n",
              " 'special': 321,\n",
              " 'bert’s': 322,\n",
              " 'encountered': 323,\n",
              " '2': 324,\n",
              " 'thing': 325,\n",
              " 'strongest': 326,\n",
              " 'between': 327,\n",
              " 'belong': 328,\n",
              " 'suggests': 329,\n",
              " 'tell': 330,\n",
              " 'should': 331,\n",
              " 'attend': 332,\n",
              " 'however': 333,\n",
              " 'identified': 334,\n",
              " '“arrow”': 335,\n",
              " 'important': 336,\n",
              " '“fruit”': 337,\n",
              " '“banana”': 338,\n",
              " 'second': 339,\n",
              " 'allow': 340,\n",
              " 'distinguish': 341,\n",
              " 'or': 342,\n",
              " 'noun': 343,\n",
              " 'depending': 344,\n",
              " 'context': 345,\n",
              " 'occurs': 346,\n",
              " 'covered': 347,\n",
              " 'take': 348,\n",
              " 'look': 349,\n",
              " 'implementing': 350,\n",
              " 'missing': 351,\n",
              " 'piece': 352,\n",
              " 'position': 353,\n",
              " 'wise': 354}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92y7gE6pj9EZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF_D",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
